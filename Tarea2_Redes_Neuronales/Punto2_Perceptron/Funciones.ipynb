{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LinearRegression(X,y,miu,iteraciones=5000):   \n",
    "    if X.ndim > 1:\n",
    "        [M,N] = X.shape\n",
    "    else:\n",
    "        M = X.shape[0]\n",
    "        N = 1\n",
    "        X = X[:,None]\n",
    "\n",
    "    VectorBias = np.ones(shape=(M,1))\n",
    "    X = np.concatenate((VectorBias,X),axis=1)\n",
    "    #print(X.shape)\n",
    "    \n",
    "    theta = np.random.rand(N+1,1)\n",
    "    for j in range(0,iteraciones):\n",
    "\n",
    "        h_x = np.dot(X,theta)\n",
    "        J = 1/(2*M)*np.dot((h_x-y).T,(h_x-y))\n",
    "        #print(J)\n",
    "\n",
    "        dJ_dtheta = np.dot((np.dot(X,theta)-y).T,X).T\n",
    "        theta = theta - miu/M * dJ_dtheta\n",
    "    ypred=(np.dot(X,theta))\n",
    "    error = np.mean(1/(2*M)*np.dot((ypred-y).T,(ypred-y)))\n",
    "    #print(theta)\n",
    "   \n",
    "    theta[1:] = theta[1:] - miu*0.1/M*theta[1:] #Regularization\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LMS(X,y,miu,iteraciones=6000):   \n",
    "    if X.ndim > 1:\n",
    "        [M,N] = X.shape\n",
    "    else:\n",
    "        M = X.shape[0]\n",
    "        N = 1\n",
    "        X = X[:,None]\n",
    "\n",
    "    VectorBias = np.ones(shape=(M,1))\n",
    "    X = np.concatenate((VectorBias,X),axis=1)\n",
    "    #print(X.shape)\n",
    "    \n",
    "    theta = np.random.rand(N+1,1)\n",
    "    for j in range(0,iteraciones):\n",
    "\n",
    "        h_x = np.dot(X,theta)\n",
    "        J = 1/(2)*np.dot((h_x-y).T,(h_x-y))\n",
    "        #print(J)\n",
    "\n",
    "        dJ_dtheta = np.dot((np.dot(X,theta)-y).T,X).T\n",
    "        theta = theta - miu * dJ_dtheta\n",
    "    ypred=(np.dot(X,theta))\n",
    "    error = np.mean(1/(2)*np.dot((ypred-y).T,(ypred-y)))\n",
    "    #print(theta)\n",
    "   \n",
    "    theta[1:] = theta[1:] - miu*0.1/M*theta[1:] #Regularization\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(X,y,alpha,lamda=0.1,iteraciones=1000):   \n",
    "    if X.ndim > 1:\n",
    "        [M,N] = X.shape\n",
    "    else:\n",
    "        M = X.shape[0]\n",
    "        N = 1\n",
    "        X = X[:,None]\n",
    "\n",
    "    VectorBias = np.ones(shape=(M,1))\n",
    "    X = np.concatenate((VectorBias,X),axis=1)\n",
    "    \n",
    "    theta = np.random.rand(N+1,1)\n",
    "    \n",
    "    for j in range(0,iteraciones):\n",
    "\n",
    "        h_x = sigmoid(np.dot(X,theta))\n",
    "        J0 = (1-y)*np.log(1-h_x)\n",
    "        J1 = y*np.log(h_x)\n",
    "\n",
    "        J0[np.isinf(J0)]=0\n",
    "        J1[np.isinf(J1)]=0\n",
    "        \n",
    "        J0 = -1/M * np.sum(J0)\n",
    "        J1 = -1/M * np.sum(J1)\n",
    "        \n",
    "        J = J1+J0\n",
    "        dJ_dtheta = 1/(M)*np.dot((h_x-y).T,X).T\n",
    "        theta = theta - alpha* dJ_dtheta\n",
    "    \n",
    "        #theta[1:] = theta[1:] - alpha*lamda/M*theta[1:] #Regularization\n",
    "    return theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def Perceptron_training(X,y,iteraciones=500,alpha=0.1,de_min=0.01):\n",
    "    \n",
    "    w_          = np.random.rand(X.shape[1] + 1,)\n",
    "    n           = X.shape[0]\n",
    "    nf          = X.shape[1]\n",
    "    sw          = 0\n",
    "    j           = 0\n",
    "    index       = []\n",
    "    error_array = []\n",
    "\n",
    "    while j < iteraciones and sw == 0:\n",
    "        for i in range(0,n):\n",
    "            g1  = np.dot(w_,np.insert(X[i,:],0,1).T)\n",
    "            yp  = sigmoid(g1)-0.5\n",
    "            if yp*y[i] < 0:\n",
    "                w_ = w_ - np.insert(X[i,:],0,1)*y[i] \n",
    "        predicted = []\n",
    "        for i in range(0,n):\n",
    "            predicted.append(Predict(X[i,:],w_))\n",
    "        E = -np.dot(predicted,y)\n",
    "        \n",
    "        index.append(j)\n",
    "        error_array.append(E[0])\n",
    "        d_error_array = derivating(error_array)\n",
    "        \n",
    "        if np.abs(d_error_array[len(d_error_array)-1]) < de_min:\n",
    "            sw = 1\n",
    "        \n",
    "        j = j + 1\n",
    "    print('FunciÃ³n de error: '+str(E[0])+' iterando ' +str(j)+' veces, con alpha: '+str(alpha))\n",
    "    \n",
    "    return [w_,predicted]\n",
    "\n",
    "def Predict(X,w_):\n",
    "    g = np.dot(w_,np.insert(X,0,1).T)\n",
    "    return sigmoid(g)-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
